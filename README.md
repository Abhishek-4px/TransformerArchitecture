# TransformerArchitecture
This contains the complete architecture of a Transformer from scratch in PyTorch . This architecture is strictly implemented according to the Research Paper Attention is all you need . You will need more google accounts and apply checkpointing in order to train on free limits of Google Colab . I've tried explaining my best via comments. WE UP!!!

## Technical Details
Dataset used - Opus books English to French
Tokenizer Used - Helinski-NLP/opus-mt-en-fr
Trained on - v8 TPU (Colab)

## Epochs Trained on - 07
1st Epoch Training Loss - 2.49               &nbsp               Val Loss - 2.49  
7th Epoch Training Loss - 0.68               &nbsp               Val Loss - 0.78


## Benchmarks :
BLEU Score - 0.4435
Rogue 1 F1 - 0.3945             &nbsp              Rogue 2 F1 - 0.1416              &nbsp              Rogue L F1 - 0.3494

