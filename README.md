# TransformerArchitecture
This contains the complete architecture of a Transformer from scratch in PyTorch . This architecture is strictly implemented according to the Research Paper Attention is all you need . You will need more google accounts and apply checkpointing in order to train on free limits of Google Colab . I've tried explaining my best via comments. WE UP!!!

## Dataset used - Opus books English to French
Tokenizer Used - Helinski-NLP/opus-mt-en-fr

## Epochs - 07
1st Epoch Training Loss - 2.49                              Val Loss - 2.49
7th Epoch Training Loss - 0.68                              Val Loss - 0.78


## Benchmarks :
BLEU Score - 0.4435
Rogue 1 F1 - 0.3945                           Rogue 2 F1 - 0.1416                            Rogue L F1 - 0.3494

